# L2 Agent Backend Configuration
# Copy this file to .env and fill in your values

# ===========================================
# SERVER CONFIGURATION
# ===========================================

# Server port (default: 3000)
PORT=3000

# Environment (development, production)
NODE_ENV=development

# Comma-separated list of allowed origins for CORS
# Use '*' for all origins (not recommended in production)
ALLOWED_ORIGINS=chrome-extension://*,http://localhost:*

# ===========================================
# LLM CONFIGURATION
# ===========================================

# URL to your LLM API endpoint
# Examples:
#   - OpenAI: https://api.openai.com/v1/chat/completions
#   - Local LLM: http://localhost:8080/api/v1/chat
#   - Custom internal API: https://your-llm-server.com/api/chat
LLM_API_URL=http://localhost:8080/api/v1/chat

# API key for LLM provider (if required)
# Leave empty if your LLM doesn't require authentication
LLM_API_KEY=your-api-key-here

# Model name to use (provider-specific)
# Examples: gpt-4, gpt-3.5-turbo, llama-2-70b-chat, etc.
LLM_MODEL=gpt-3.5-turbo

# Request timeout in milliseconds (default: 30000)
LLM_TIMEOUT=30000

# Optional: Additional headers for LLM API requests (JSON format)
# LLM_HEADERS={"X-Custom-Header": "value"}

# Optional: Default parameters for LLM requests (JSON format)
# LLM_DEFAULT_PARAMS={"temperature": 0.7, "max_tokens": 2000}
